{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPScZQlKodLLPFIRDDCa0Ex",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ilhom-Utkirov/data_science/blob/main/librosa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-rJb4K3d19D"
      },
      "outputs": [],
      "source": [
        "# **Uploading dataset**\n",
        "\n",
        "\n",
        "# Import and unzip dataset\n",
        "!wget -cq https://github.com/dbdmg/data-science-lab/raw/master/datasets/free-spoken-digit.zip\n",
        "!unzip free-spoken-digit.zip\n",
        "\n",
        "# **Features and labels creation**\n",
        "\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "'''Using librosa can be slower, but provides normalization of the data and a\n",
        "new standard sample rate to each file\n",
        "'''\n",
        "\n",
        "files = glob.glob('dev' + '/*.wav')\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for f in files:\n",
        "  data, sample_rate = librosa.load(f)\n",
        "  X.append(data)\n",
        "  label = int(f.split(\"_\")[1].split(\".\")[0])\n",
        "  y.append(label)\n",
        "\n",
        "print(len(X[0]))\n",
        "\n",
        "# **Preprocessing**\n",
        "\n",
        "# For this step I used **Mel-Frequency Cepstral Coefficients (MFCC)**, which involve taking the whole normalized (-1, 1) data set and calculates the fourier transform on each signal. After this it is transferred to the mel scale (a scale based on the non-linear way humans percieve a sound) via a logaritmic transformation. Finally, we take the average the data in every range in the scale (which is based in the number of scales we want to use. 20 in this case). This way we have the data features preprocessed and with the same length.\n",
        "\n",
        "def preprocess(data, sample_rate):\n",
        "  mfcc = [ librosa.feature.mfcc(x, sample_rate) for x in data ]\n",
        "  data_pre = [ np.mean(x.T, axis=0) for x in mfcc ]\n",
        "  return data_pre\n",
        "\n",
        "X_p = preprocess(X, sample_rate)\n",
        "\n",
        "print(X_p[:5])\n",
        "\n",
        "# Some non tuned classifiers to decide on which to perform hyper-parameter tunning later on. the best performer will be chosen for this.\n",
        "\n",
        "# Vanilla Classifiers\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_p, y, test_size=0.2)\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "y_val_pred = clf.predict(X_val)\n",
        "\n",
        "p, r, f1, s = precision_recall_fscore_support(y_val, y_val_pred)\n",
        "print(\"Vanilla RandomForest:\", f1.mean())\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_val_pred = clf.predict(X_val)\n",
        "\n",
        "p, r, f1, s = precision_recall_fscore_support(y_val, y_val_pred)\n",
        "print(\"Vanilla SVC:\", f1.mean())\n",
        "\n",
        "# **Hyper-parameters Tunning**\n",
        "\n",
        "# Grid search based hyper-parameter tunning on the SVC.\n",
        "\n",
        "# Grid Search SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Set the parameters by cross-validation\n",
        "tuned_parameters = [\n",
        "    {\"kernel\": [\"rbf\"], \"gamma\": ['scale','auto'], \"C\": [ 1, 10, 100, 1000 ]},\n",
        "\n",
        "    {\"kernel\": [\"sigmoid\"], \"C\": [1, 10, 100, 1000]},\n",
        "]\n",
        "\n",
        "print(\"# Tuning hyper-parameters for F1-macro\")\n",
        "print()\n",
        "\n",
        "clf = GridSearchCV(SVC(), tuned_parameters, scoring=\"f1_macro\")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters set found on development set:\")\n",
        "print()\n",
        "print(clf.best_params_)\n",
        "print()\n",
        "print(\"Grid scores on development set:\")\n",
        "print()\n",
        "means = clf.cv_results_[\"mean_test_score\"]\n",
        "stds = clf.cv_results_[\"std_test_score\"]\n",
        "for mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
        "print()\n",
        "\n",
        "print(\"Detailed classification report:\")\n",
        "print()\n",
        "print(\"The model is trained on the full development set.\")\n",
        "print(\"The scores are computed on the full evaluation set.\")\n",
        "print()\n",
        "y_true, y_pred = y_val, clf.predict(X_val)\n",
        "print(classification_report(y_true, y_pred))\n",
        "print()\n",
        "\n",
        "# Best parameters implemented in th/e model\n",
        "\n",
        "# Tuned Classifiers\n",
        "\n",
        "best_params = {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
        "\n",
        "svc = make_pipeline(StandardScaler(), SVC(gamma='auto', C=10))\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "y_val_pred_svc = svc.predict(X_val)\n",
        "\n",
        "p, r, f1, s = precision_recall_fscore_support(y_val, y_val_pred_svc)\n",
        "print(\"Vanilla SVC:\", f1.mean())\n",
        "\n",
        "# **Test Phase**. Here we test the trained classifier on never-seen-before data. As always we extract and preprocess the data.\n",
        "\n",
        "files = glob.glob('eval' + '/*.wav')\n",
        "file_ids = [ int(f.split('/')[1].split('.')[0]) for f in files ]\n",
        "X_test = []\n",
        "\n",
        "for f in files:\n",
        "  data, sample_rate = librosa.load(f)\n",
        "  X_test.append(data)\n",
        "\n",
        "X_test = preprocess(X_test, sample_rate)\n",
        "\n",
        "y_test = svc.predict(X_test)\n",
        "\n",
        "print(y_test[:5])\n",
        "print(files[:5])\n",
        "print(file_ids[:5])\n",
        "\n",
        "# Here we merge both, the id of the files and the results, into a single Numpy array that we later on convert into a DataFrame\n",
        "\n",
        "f_w = np.stack((file_ids, y_test), axis=1)\n",
        "sorted_w = f_w[f_w[:,0].argsort()]\n",
        "\n",
        "print(f_w[:5])\n",
        "print(f_w.shape)\n",
        "print(sorted_w[:5])\n",
        "\n",
        "# By converting the Numpy array into a Pandas' DataFrame we are able to easily write the .csv for testing.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data=sorted_w, columns=['Id', 'Predicted'])\n",
        "df.to_csv('results.csv',index=False)"
      ]
    }
  ]
}